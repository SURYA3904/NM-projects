from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def generate_text(prompt, max_length=150, temperature=0.8, top_p=0.9, top_k=50):
    model_name = "gpt2-medium"  # Use 'gpt2' if resources are limited
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)

    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    output = model.generate(
        input_ids,
        max_length=max_length,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text

if __name__ == "__main__":
    print("üìÑ General Content Generator (GPT-2 Based)")
    prompt = input("Enter your topic or prompt: ").strip()
    
    if not prompt.endswith((".", "?", "!", ":")):
        prompt += ":"

    print("\nGenerating content, please wait...\n")
    result = generate_text(prompt)
    print("üìù Generated Content:\n")
    print(result)
